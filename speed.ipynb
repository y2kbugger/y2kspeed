{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "from time import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blur/focus based\n",
    "def variance_of_laplacian(image):\n",
    "    # compute the Laplacian of the image and then return the focus\n",
    "    # measure, which is simply the variance of the Laplacian\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def tile(im, nrows=1, ncolumns=6):\n",
    "    M = im.shape[0] // nrows\n",
    "    N = im.shape[1] // ncolumns\n",
    "    rows = []\n",
    "    for y in range(0,N*ncolumns,N):\n",
    "        row = []\n",
    "        for x in range(0,M*nrows,M):\n",
    "            row.append(im[x:x+M,y:y+N])\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def via_variance_of_laplacian(f):\n",
    "    image = f['_']\n",
    "    tiles = tile(image, 8, 20)\n",
    "    return [variance_of_laplacian(i) for i in flatten(tiles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optical Flow Based\n",
    "def fix_perspective(im, debugging=False):\n",
    "    h, w = im.shape\n",
    "    assert (w, h) == (640, 160) # this is tuned for a very specific crop and dashcam position\n",
    "    left = 60 # left-right adjustment\n",
    "    top = 5\n",
    "    bottom = 30\n",
    "    if debugging:\n",
    "        src_rect = np.array([\n",
    "            [245, originy+top],   [370, originy+top],\n",
    "            [0, 125],   [600, 100]],\n",
    "            dtype = \"float32\")\n",
    "        dst_rect = np.array([\n",
    "            [80-left, 0],    [330-left, 0],\n",
    "            [108-left, 840],  [320-left, 800]],\n",
    "            dtype = \"float32\")\n",
    "        M = cv2.getPerspectiveTransform(src_rect, dst_rect)\n",
    "        print(repr(M))\n",
    "    else:\n",
    "        M = np.array(\n",
    "           [[-5.79976346e+00, -2.25571424e+01,  1.92672659e+03],\n",
    "            [-1.81898940e-14, -1.56260338e+02,  3.90650844e+03],\n",
    "            [ 5.42171076e-05, -1.56819369e-01,  1.00000000e+00]])\n",
    "    dst = cv2.warpPerspective(im,M,(300,840-bottom))\n",
    "    if debugging:\n",
    "        plt.rcParams['figure.figsize'] = [20, 12]\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        plt.imshow(dst)\n",
    "        plt.show()\n",
    "    return dst\n",
    "\n",
    "def optical_flow(im, last_im, debugging=False):\n",
    "   \n",
    "    # params for ShiTomasi corner detection\n",
    "    feature_params = dict(\n",
    "        maxCorners = 100,\n",
    "        qualityLevel = 0.007,\n",
    "        minDistance = 20,\n",
    "        blockSize = 9,\n",
    "        #useHarrisDetector = True,\n",
    "        )\n",
    "\n",
    "    # Parameters for lucas kanade optical flow\n",
    "    lk_params = dict(\n",
    "        winSize  = (15,15),\n",
    "        maxLevel = 1,\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03),\n",
    "        )\n",
    "    \n",
    "    p0 = cv2.goodFeaturesToTrack(last_im, mask = None, **feature_params)\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(last_im, im, p0, None, **lk_params)\n",
    "\n",
    "    # Create a mask image for drawing purposes\n",
    "    mask = np.zeros_like(last_im)\n",
    "    \n",
    "    # Select good points\n",
    "    good_new = p1[st==1]\n",
    "    good_old = p0[st==1]\n",
    "    \n",
    "    \n",
    "    # setup zenith\n",
    "    shape=im.shape\n",
    "    #ys,xs = np.indices(shape)\n",
    "    x_offset = shape[1] // 2\n",
    "    y_offset = int(originy)\n",
    "    \n",
    "    img = im.copy()\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    # draw the tracks\n",
    "    for i,(new,old) in enumerate(zip(good_new,good_old)):\n",
    "        x1, y1 = old.ravel()\n",
    "        x2, y2 = new.ravel()\n",
    "        \n",
    "        dx, dy = (x2-x1), (y2-y1)\n",
    "        if dx == 0.0:\n",
    "            dx = 0.00001 #prevent divide by zero\n",
    "        Vf_slope = dy/dx\n",
    "        Vf_mag = (dx**2 + dy**2)**0.5\n",
    "        \n",
    "        if Vf_mag < 3 or Vf_mag>120:\n",
    "            #not moved enough\n",
    "            continue\n",
    "\n",
    "        df.loc[i,'|Vf|'] = Vf_mag\n",
    "        df.loc[i,'Vf_slope'] = Vf_slope\n",
    "        \n",
    "        right_direction = y2>y1 and abs(Vf_slope) > 3 # down and steep\n",
    "        df.loc[i,'right_direction'] = right_direction\n",
    "        \n",
    "        if right_direction:\n",
    "            color=255\n",
    "            df.loc[i,'good'] = True\n",
    "        else:\n",
    "            color=130\n",
    "            df.loc[i,'good'] = False\n",
    "        \n",
    "        if debugging:\n",
    "            img = cv2.arrowedLine(img, (int(x1),int(y1)),(int(x2),int(y2)), color, tipLength=.3)\n",
    "            img = cv2.circle(img,(int(x1),int(y1)),2,color, -1)\n",
    "    \n",
    "    if len(df) != 0:\n",
    "        # filter out noisy \"small\" flow vectors\n",
    "        Vf_max = df[df['good']==True]['|Vf|'].max()\n",
    "        df.loc[df['|Vf|']<Vf_max*.45,'good'] = False\n",
    "    \n",
    "    if debugging:\n",
    "        if len(df) == 0:\n",
    "            print(\"no useful points\")\n",
    "        else:\n",
    "            display(df.sort_values(by='|Vf|'))       \n",
    "            bins = list(range(0,101,10))\n",
    "            plt.rcParams['figure.figsize'] = [20, 5]\n",
    "            df['|Vf|'].hist(bins=bins)\n",
    "            df[df['good']==True]['|Vf|'].hist(bins=bins)\n",
    "            plt.show()\n",
    "\n",
    "        plt.rcParams['figure.figsize'] = [20, 12]\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "    if len(df) == 0:\n",
    "        return [np.nan, np.nan]\n",
    "    else:\n",
    "        return [\n",
    "            df.loc[df['good'],'|Vf|'].mean(),\n",
    "            df.loc[df['right_direction'],'|Vf|'].std(),\n",
    "            ]\n",
    "\n",
    "def via_lk_optical_flow(f):\n",
    "    im, last_im = f['_'], f['1']\n",
    "    return optical_flow(im, last_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process frames\n",
    "def frames():\n",
    "    vidcap = cv2.VideoCapture('../data/train.mp4')\n",
    "    while True:\n",
    "        success, image = vidcap.read()\n",
    "        if success:\n",
    "            yield {'orig': image, '_': image, 'xs':[]}\n",
    "        else:\n",
    "            return\n",
    "\n",
    "originy=None\n",
    "def crop(image, bottom=100, top=220):\n",
    "    # take of top and bottom\n",
    "    global originy\n",
    "    originy = image.shape[0] / 2 - top\n",
    "    return image[top:image.shape[0] - bottom,:]\n",
    "\n",
    "def crop_frames(frames):\n",
    "    for f in frames:\n",
    "        f['_'] = crop(f['_'])\n",
    "        yield f\n",
    "\n",
    "def bw_frames(frames):\n",
    "    for f in frames:\n",
    "        f['_'] = cv2.cvtColor(f['_'],cv2.COLOR_BGR2GRAY)\n",
    "        yield f \n",
    "\n",
    "def perspec_frames(frames):\n",
    "    for f in frames:\n",
    "        f['_'] = fix_perspective(f['_'])\n",
    "        yield f\n",
    "        \n",
    "def diff_frames(frames):\n",
    "    #from skimage.measure import compare_ssim\n",
    "    for f in frames:\n",
    "        f['diff'] = cv2.subtract(f['_'],f['1'])\n",
    "        #(score, diff) = compare_ssim(grayA, grayB, full=True)\n",
    "        yield f\n",
    "\n",
    "def adjacent(frames):\n",
    "    # add \"memory\" in keys '1', '2', ... for previous frames\n",
    "    # repeats at the end to keep length len\n",
    "    fs = list()\n",
    "    count = 2\n",
    "    \n",
    "    def _updated_f():\n",
    "        f = fs.pop(0)\n",
    "        f.update({str(n+1):f['_'] for n, f in enumerate(fs)})\n",
    "        return f \n",
    "        \n",
    "    for f in frames:\n",
    "        fs.append(f)\n",
    "        if len(fs) > count:\n",
    "            yield _updated_f()\n",
    "\n",
    "    for _ in range(count):\n",
    "        fs.append(f)\n",
    "        yield _updated_f()\n",
    "        \n",
    "def print_keys(frames):\n",
    "    for f in frames:\n",
    "        print(repr(list(f.keys())))\n",
    "        yield f\n",
    "\n",
    "def view_frames(frames):\n",
    "    for f in frames:\n",
    "        for k in f.keys():\n",
    "            cv2.imshow(k,f[k])\n",
    "        try:\n",
    "            cv2.waitKey(0)\n",
    "        except KeyboardInterrupt:\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"Stopping early, KeyboardInterrupt\")\n",
    "            return\n",
    "        yield f\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def analyze_frames(via):\n",
    "    def analyze(frames):\n",
    "        for f in frames:\n",
    "            f['xs'] += via(f)\n",
    "            yield f\n",
    "    return analyze\n",
    "\n",
    "class FeatureExtractor():\n",
    "    def __init__(self, frames_generator_maker):\n",
    "        self._frames = frames_generator_maker\n",
    "        self._steps = []\n",
    "    def add_step(self, step):\n",
    "        if callable(step):\n",
    "            self._steps.append(step)\n",
    "    def __iter__(self):\n",
    "        pipeline = self._frames()\n",
    "        for s in self._steps:\n",
    "            pipeline = s(pipeline)\n",
    "        return pipeline\n",
    "    def _pprogress(self, count, force=False):\n",
    "        if force or time()-self._last>30:\n",
    "            self._last = time()\n",
    "            print(f\"{count+1} processed in {(time()-self._start)/60:2.1f} minutes\")\n",
    "    def extract_features(self):\n",
    "        self._start = time()\n",
    "        self._last = self._start\n",
    "        X = []\n",
    "        i=0\n",
    "        for i, f in enumerate(self):\n",
    "            X.append(f['xs'])\n",
    "            self._pprogress(i)\n",
    "        self._pprogress(i,True)\n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 processed in 0.0 minutes\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureExtractor(frames)\n",
    "#fe.add_step(lambda g: islice(g, 17500, 20400, 1)) # limit frames (start, stop, step)\n",
    "#fe.add_step(lambda g: islice(g, 10)) # limit frames (start, stop, step)\n",
    "fe.add_step(crop_frames)\n",
    "fe.add_step(bw_frames)\n",
    "fe.add_step(perspec_frames)\n",
    "fe.add_step(adjacent)\n",
    "fe.add_step(analyze_frames(via_lk_optical_flow))\n",
    "#fe.add_step(analyze_frames(via_variance_of_laplacian))\n",
    "\n",
    "#fe.add_step(print_keys)\n",
    "#fe.add_step(diff_frames)\n",
    "#fe.add_step(view_frames)\n",
    "\n",
    "xs = fe.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18.494126158664322, 8.29084932597797],\n",
       " [9.905680029277349, 2.7572038061826816],\n",
       " [nan, nan],\n",
       " [19.848947963442235, 6.390776480312567],\n",
       " [67.95310829589732, 25.65783939203523],\n",
       " [27.97024008561632, 12.143982826278608],\n",
       " [12.27375792966662, 4.7162991033174935],\n",
       " [14.58135421179583, 7.048165041231827],\n",
       " [8.629345782087823, nan],\n",
       " [nan, nan]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_time_in_chunks(df, n):\n",
    "    \"\"\"Break df into n-lengths mini dfs\"\"\"\n",
    "    assert len(df) >= n*10, \"doesn't meet minimum number of chunks\"\n",
    "    assert (len(df) % n) == 0, \"all chunks equal size\"\n",
    "    \n",
    "    chunk_count = len(df[0]) // n\n",
    "    chunks = []\n",
    "    for x in range(0, len(df), n):\n",
    "        chunks.append(df[x:x + n])\n",
    "    random.shuffle(chunks)\n",
    "    print(f\"Using {len(chunks):0d} chunks\")\n",
    "    return pd.concat(chunks, axis=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "doesn't meet minimum number of chunks",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c33cce2742b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2040\u001b[0m \u001b[0;31m# ten chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m204\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;31m# 50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mXy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_time_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mXy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6abf0505188b>\u001b[0m in \u001b[0;36mshuffle_time_in_chunks\u001b[0;34m(df, n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshuffle_time_in_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Break df into n-lengths mini dfs\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"doesn't meet minimum number of chunks\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all chunks equal size\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: doesn't meet minimum number of chunks"
     ]
    }
   ],
   "source": [
    "y = pd.read_csv('../data/train.txt', header=None)\n",
    "X = pd.DataFrame(xs)\n",
    "X.fillna(method='pad', inplace=True)\n",
    "\n",
    "Xy = X.copy(deep=True)\n",
    "Xy['y'] = y\n",
    "\n",
    "chunksize = 60\n",
    "chunksize = 2040 # ten chunks\n",
    "chunksize = 204*2 # 50\n",
    "Xy = shuffle_time_in_chunks(Xy, chunksize)\n",
    "Xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_testcount(test_df, fraction_testset=0.3):\n",
    "    chunks = 1\n",
    "    while True:\n",
    "        chunks += 1\n",
    "        testcount = chunksize * chunks\n",
    "        if testcount/len(test_df)>fraction_testset:\n",
    "            break\n",
    "    print(f\"using testcount = {testcount}\")\n",
    "    return testcount\n",
    "    \n",
    "testcount = find_testcount(Xy, .43)\n",
    "\n",
    "Xy_train, Xy_test = train_test_split(Xy, test_size=testcount,shuffle=False)\n",
    "Xy_train, Xy_test = Xy_train.copy(deep=True), Xy_test.copy(deep=True)\n",
    "print(\"Percent test    =\", testcount/len(Xy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "m_dum = DummyRegressor(strategy='mean')\n",
    "\n",
    "m_ensemble = StackingRegressor([\n",
    "   ('svr', SVR(C=20)),\n",
    "   ('rf', RandomForestRegressor()),\n",
    "   ('mlp', MLPRegressor(shuffle=False, alpha=0.1)),\n",
    "   ])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(interaction_only=False, include_bias=False)),\n",
    "    #'reduce_dims', PCA(n_components=4)),\n",
    "    #'svr', SVR(C=20)),\n",
    "    #('rf', RandomForestRegressor()),\n",
    "    #'mlp', MLPRegressor(shuffle=False, alpha=0.1)),\n",
    "    #('br', BayesianRidge()),\n",
    "    #('stack', m_ensemble),\n",
    "    ('lin', LinearRegression()),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# # Training classifiers\n",
    "# reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\n",
    "# reg2 = RandomForestRegressor(random_state=1, n_estimators=10)\n",
    "# reg3 = LinearRegression()\n",
    "# ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "# BaysianRidge\n",
    "# MLP NN\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    #'svr__C':[.01, .1, .3, .9, 2, 4, 10, 20],\n",
    "    #'svr__C':[10, 20, 30, 40, 70, 120],\n",
    "    'mlp__alpha':[0.0001, 0.001, 0.01],\n",
    "    #'svr__C':[.3],RandomForestRegressor\n",
    "    #'svr__gamma':[0.006/4, 0.006/2,0.006,0.006*2],\n",
    "    }\n",
    "grid = GridSearchCV(pipe, parameters, verbose=10, n_jobs=5, cv=5)\n",
    "\n",
    "m_real = pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dum.fit(Xy_train[X.columns], Xy_train['y'])\n",
    "m_real.fit(Xy_train[X.columns], Xy_train['y'])\n",
    "try:\n",
    "    print(m_real.best_estimator_)\n",
    "except:\n",
    "    print(m_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate full original df\n",
    "Xy_test.loc[:,'is_test'] = True\n",
    "Xy_train.loc[:,'is_test'] = False\n",
    "Xy2 = pd.concat([Xy_test,Xy_train]).sort_index()\n",
    "\n",
    "# get predictions\n",
    "Xy2['dum'] = m_dum.predict(Xy2[X.columns])\n",
    "Xy2['pred'] = m_real.predict(Xy2[X.columns])\n",
    "\n",
    "# for convience\n",
    "Xy2_test_ix = Xy2['is_test'] == True\n",
    "Xy2_train_ix = Xy2['is_test'] == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smooth based on distribution of acceleration in training set\n",
    "idea use kalman filter based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune a kalman filter\n",
    "from pykalman import KalmanFilter\n",
    "em_vars = [\n",
    "     #'transition_covariance',\n",
    "     'observation_covariance',\n",
    "     'initial_state_mean', 'initial_state_covariance']\n",
    "\n",
    "T = np.array([[.00035]]) # smaller is more resistance to acceleration\n",
    "\n",
    "kf = KalmanFilter(initial_state_mean=0, n_dim_obs=1, transition_covariance=T)\n",
    "kf_tuned = kf.em(Xy2.loc[Xy2_train_ix,'y'].values, n_iter=0, em_vars=em_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# never below 0\n",
    "Xy2.loc[Xy2['pred']<0.0,'pred'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply kalman\n",
    "Xy2.loc[Xy2_test_ix,'pred_kf'] = kf_tuned.smooth(Xy2.loc[Xy2_test_ix,'pred'].values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy2['0_kf'] = kf_tuned.smooth(Xy2[0].values)[0]\n",
    "Xy2['0_kf'] *= Xy2['y'].mean()/Xy2['0_kf'].mean()\n",
    "\n",
    "Xy2['1_kf'] = kf_tuned.smooth(Xy2[1].values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [100, 12]\n",
    "plt.gca().set_xlim((0,len(Xy2)))\n",
    "def plot_Xy2(ix_mask, column, **kwargs):\n",
    "    if ix_mask is None:\n",
    "        df = Xy2.loc[:,column]\n",
    "    else:\n",
    "        df = Xy2.loc[ix_mask, column]\n",
    "    plt.plot(df.index, df.values, **kwargs)\n",
    "    \n",
    "plot_Xy2(Xy2_test_ix, 'pred', marker='o', linewidth=0.0, color='green', alpha=.1)\n",
    "plot_Xy2(Xy2_test_ix, 'pred_kf', marker='o', linewidth=0.0, color='green', alpha=.1)\n",
    "plot_Xy2(Xy2_train_ix, 'pred', marker='o', linewidth=0.0, color='purple', alpha=.1)\n",
    "plot_Xy2(None, 0, marker='o', linewidth=0.0, color='yellow', alpha=.1)\n",
    "plot_Xy2(None, '0_kf', marker='o', linewidth=0.0, color='orange', alpha=.1)\n",
    "plot_Xy2(None, '1_kf', marker='', linewidth=1.0, color='blue')\n",
    "plot_Xy2(None, 'y', marker='', linewidth=1.4, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_line(ix_mask, column='dum'):\n",
    "    if ix_mask is not None:\n",
    "        Xy = Xy2[ix_mask]\n",
    "    else:\n",
    "        Xy = Xy2\n",
    "    err = mean_squared_error(Xy['y'], Xy[column])\n",
    "    print(f\"{err:0.1f}\", end=' ')\n",
    "\n",
    "print(f\"dummy test train test_kf\")\n",
    "print_summary_line(Xy2_test_ix,'dum')\n",
    "print_summary_line(Xy2_test_ix,'pred')\n",
    "print_summary_line(Xy2_train_ix,'pred')\n",
    "print_summary_line(Xy2_test_ix,'pred_kf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
