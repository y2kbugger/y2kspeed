{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "from time import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "from scipy import ndimage, stats\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blur/focus based\n",
    "def variance_of_laplacian(image):\n",
    "    # compute the Laplacian of the image and then return the focus\n",
    "    # measure, which is simply the variance of the Laplacian\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def tile(im, nrows=1, ncolumns=6, debugging=False):\n",
    "    M = im.shape[0] // nrows\n",
    "    N = im.shape[1] // ncolumns\n",
    "    rows = []\n",
    "    for x in range(0, M*nrows,M):\n",
    "        row = []\n",
    "        for y in range(0,N*ncolumns,N):\n",
    "            row.append(im[x:x+M,y:y+N])\n",
    "        rows.append(row)\n",
    "    if debugging:\n",
    "        width = 3.0\n",
    "        height = width/im.shape[1]*im.shape[0]\n",
    "        fig = plt.figure(figsize = (width,height))\n",
    "        gs = gridspec.GridSpec(nrows, ncolumns, figure=fig)\n",
    "        gs.update(wspace=0.0, hspace=0.0)\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncolumns):\n",
    "                ax = fig.add_subplot(gs[r, c])\n",
    "                ax.imshow(rows[r][c], vmin=0, vmax=im.max())\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        plt.show()\n",
    "            \n",
    "    return rows\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def via_variance_of_laplacian(f):\n",
    "    image = f['_']\n",
    "    tiles = tile(image, nrows=3, ncolumns=1)\n",
    "    return [variance_of_laplacian(i) for i in flatten(tiles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optical Flow Based\n",
    "def fix_perspective(im, debugging=False):\n",
    "    h, w = im.shape\n",
    "    assert (w, h) == (640, 160) # this is tuned for a very specific crop and dashcam position\n",
    "    left = 60 # left-right adjustment\n",
    "    top = 5\n",
    "    bottom = 30\n",
    "    if debugging:\n",
    "        src_rect = np.array([\n",
    "            [245, originy+top],   [370, originy+top],\n",
    "            [0, 125],   [600, 100]],\n",
    "            dtype = \"float32\")\n",
    "        dst_rect = np.array([\n",
    "            [80-left, 0],    [330-left, 0],\n",
    "            [108-left, 840],  [320-left, 800]],\n",
    "            dtype = \"float32\")\n",
    "        M = cv2.getPerspectiveTransform(src_rect, dst_rect)\n",
    "        print(repr(M))\n",
    "    else:\n",
    "        M = np.array(\n",
    "           [[-5.79976346e+00, -2.25571424e+01,  1.92672659e+03],\n",
    "            [-1.81898940e-14, -1.56260338e+02,  3.90650844e+03],\n",
    "            [ 5.42171076e-05, -1.56819369e-01,  1.00000000e+00]])\n",
    "    dst = cv2.warpPerspective(im,M,(300,840-bottom))\n",
    "    if debugging:\n",
    "        plt.rcParams['figure.figsize'] = [20, 12]\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        plt.imshow(dst)\n",
    "        plt.show()\n",
    "    return dst\n",
    "\n",
    "def mutating_base_calcs(df, n):\n",
    "    df['dx'] = df.x2 - df.x1 + 0.00001\n",
    "    df['dy'] = (df.y2 - df.y1)/n\n",
    "    df['Vf_slope'] = df.dy/df.dx\n",
    "    df['|Vf|'] = np.sqrt(df.dx**2 + df.dy**2)\n",
    "\n",
    "    df['right_direction'] = (df.y2>df.y1) & (abs(df.Vf_slope) > 3) # down and steep\n",
    "    df['good'] = df['right_direction']\n",
    "\n",
    "def analyze_lk_optical_flow_dfs(dfs):\n",
    "    xs = []\n",
    "    def analyze_df(df):\n",
    "        nonlocal xs\n",
    "        # absurd\n",
    "        #df['good'] = df['good'] & (df['|Vf|'] > 2.5)\n",
    "        df['good'] = df['good'] & (df['|Vf|'] < (35/.45)) #25 is data set max, .45 coverts from Vf to velocity\n",
    "        \n",
    "        if sum(df['good']==True) == 0:\n",
    "            xs += [np.nan, np.nan]\n",
    "        else:\n",
    "            with np.errstate(divide='ignore',invalid='ignore'):\n",
    "                df.loc[df['good'],'z'] = stats.zscore(df.loc[df['good'],'|Vf|'])\n",
    "            df.loc[df['good']==False,'z'] = 100.0\n",
    "            df['good'] = df['good'] & (df['z'] < 1.7)\n",
    "\n",
    "            if len(df) != 0:\n",
    "                # filter out noisy \"small\" flow vectors\n",
    "                Vf_max_good = df[df['good']==True]['|Vf|'].max()\n",
    "                df['good'] = df['good'] & (df['|Vf|'] > Vf_max_good * 0.25)\n",
    "            xs.append(df.loc[df['good'],'|Vf|'].mean())\n",
    "            xs.append(df.loc[df['good'],'|Vf|'].std())\n",
    "        xs.append(df['dy'].mean())\n",
    "    overall = 0\n",
    "    for n, df in enumerate(dfs):\n",
    "        global N\n",
    "        N = n + 1\n",
    "        mutating_base_calcs(df,N)\n",
    "        analyze_df(df)\n",
    "        \n",
    "        overall += df.loc[df['good'],'|Vf|'].mean()/(len(dfs)+1)\n",
    "    \n",
    "    stacked_df = pd.concat(dfs, ignore_index=True)\n",
    "    analyze_df(stacked_df)\n",
    "    overall += stacked_df.loc[stacked_df['good'],'|Vf|'].mean()/(len(dfs)+1)\n",
    "    \n",
    "    xs.append(overall)\n",
    "\n",
    "    # 0 1 2 #n=1\n",
    "    # 3 4 5 #n=2\n",
    "    # 6 7 8 #n=3\n",
    "    # 9 10 11 #stacked\n",
    "    # 12 #overall\n",
    "    return xs\n",
    "\n",
    "def via_lk_optical_flow_multi(frame, count=3):\n",
    "    frame['optical_flow'] = []\n",
    "    dfs = []\n",
    "    for i in range(1, count+1):\n",
    "        dfs.append(optical_flow(frame['_'], frame[str(i)], frame))\n",
    "    return analyze_lk_optical_flow_dfs(dfs)\n",
    "\n",
    "def debug_optical(df,img):\n",
    "    img = image_next.copy()\n",
    "    # we lose old arrow debugging with the df approach, sorry\n",
    "    #         if right_direction:\n",
    "    #             color=255\n",
    "    #             df.loc[i,'good'] = True\n",
    "    #         else:\n",
    "    #             color=130\n",
    "    #             df.loc[i,'good'] = False\n",
    "\n",
    "    #         if debugging:\n",
    "    #             img = cv2.arrowedLine(img,(int(x1),int(y1)),(int(x2),int(y2)), color, tipLength=.3)\n",
    "    #             img = cv2.circle(img,(int(x1),int(y1)),2,color, -1)\n",
    "    if len(df) == 0:\n",
    "        print(\"no useful points\")\n",
    "    else:\n",
    "        display(df.sort_values(by='|Vf|'))       \n",
    "        bins = list(range(0,101,10))\n",
    "        plt.rcParams['figure.figsize'] = [20, 5]\n",
    "        df['|Vf|'].hist(bins=bins)\n",
    "        df[df['good']==True]['|Vf|'].hist(bins=bins)\n",
    "        plt.show()\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [20, 12]\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "# params for ShiTomasi corner detection\n",
    "feature_params = dict(\n",
    "    maxCorners = 100,\n",
    "    qualityLevel = 0.007,\n",
    "    minDistance = 20,\n",
    "    blockSize = 9,\n",
    "    #useHarrisDetector = True,\n",
    "    )\n",
    "\n",
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict(\n",
    "    winSize  = (15,15),\n",
    "    maxLevel = 1,\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03),\n",
    "    )\n",
    "\n",
    "def optical_flow(image, image_next, frame, debugging=False):\n",
    "    p0 = cv2.goodFeaturesToTrack(image, mask=None, **feature_params)\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(image, image_next, p0, None, **lk_params)\n",
    "    \n",
    "    points = []\n",
    "    for new, old in zip(p1[st==1],p0[st==1]):\n",
    "        x1, y1 = old.ravel()\n",
    "        x2, y2 = new.ravel()\n",
    "        points.append((x1,x2,y1,y2))\n",
    "    \n",
    "    df = pd.DataFrame(data=points, columns=('x1','x2','y1','y2'))\n",
    "    if debugging:\n",
    "        debug_optical(df,img)\n",
    "\n",
    "    frame['optical_flow'].append(df)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff based for stop detect\n",
    "def via_diff(f):\n",
    "    return [abs(cv2.subtract(f['_'],f['1']).sum())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Process frames\n",
    "def frames(file='../data/train.mp4'):\n",
    "    vidcap = cv2.VideoCapture(file)\n",
    "    while True:\n",
    "        success, image = vidcap.read()\n",
    "        if success:\n",
    "            yield {'orig': image, '_': image, 'xs':[]}\n",
    "        else:\n",
    "            return\n",
    "\n",
    "originy=None\n",
    "def crop(image, bottom=100, top=220):\n",
    "    # take of top and bottom\n",
    "    global originy\n",
    "    originy = image.shape[0] / 2 - top\n",
    "    return image[top:image.shape[0] - bottom,:]\n",
    "\n",
    "def lookahead(frames, count=3):\n",
    "    # add \"lookahead\" in keys '1', '2', ...\n",
    "    # repeats at the end to keep length len\n",
    "    fs = list()\n",
    "    \n",
    "    def _updated_f():\n",
    "        f = fs.pop(0)\n",
    "        f.update({str(n+1):f['_'] for n, f in enumerate(fs)})\n",
    "        return f \n",
    "        \n",
    "    for f in frames:\n",
    "        fs.append(f)\n",
    "        if len(fs) > count:\n",
    "            yield _updated_f()\n",
    "\n",
    "    for _ in range(count):\n",
    "        fs.append(f)\n",
    "        yield _updated_f()\n",
    "        \n",
    "def print_frame_keys(frames):\n",
    "    for f in frames:\n",
    "        print(repr(list(f.keys())))\n",
    "        yield f\n",
    "\n",
    "def view_frames(frames):\n",
    "    for f in frames:\n",
    "        for k in f.keys():\n",
    "            try:\n",
    "                cv2.imshow(k,f[k])\n",
    "            except:\n",
    "                pass\n",
    "        try:\n",
    "            cv2.waitKey(0)\n",
    "        except KeyboardInterrupt:\n",
    "            cv2.destroyAllWindows()\n",
    "            print(\"Stopping early, KeyboardInterrupt\")\n",
    "            return\n",
    "        yield f\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def persist_raw_optical_flow(frames):\n",
    "    import os\n",
    "    path = f'./data/{int(time())}'\n",
    "    os.makedirs(path)\n",
    "    for i, f in enumerate(frames):\n",
    "        for j, df in enumerate(f['optical_flow']):\n",
    "            df: pd.DataFrame\n",
    "            df.to_pickle(os.path.join(path,f'{i}_{j}.pkl'))\n",
    "        yield f\n",
    "\n",
    "class FeatureExtractor():\n",
    "    def __init__(self, frames_generator_maker):\n",
    "        self._frames = frames_generator_maker\n",
    "        self._steps = []\n",
    "    def add_step(self, step):\n",
    "        \"\"\"step(frames_iterator) yields-> [frame,frame,...]; you can filter or gather frames\"\"\"\n",
    "        if callable(step):\n",
    "            self._steps.append(step)\n",
    "    def add_processor(self, processor):\n",
    "        \"\"\"processor(img) returns-> img; frame['_'] is mutated\"\"\"\n",
    "        def _step(frames):\n",
    "            for f in frames:\n",
    "                f['_'] = processor(f['_'])\n",
    "                yield f\n",
    "        self.add_step(_step)\n",
    "    def add_analyzer(self, analyzer):\n",
    "        \"\"\"analyzer(frame) returns-> [x1,x2,...]; frame['_'] is forwarded untouched, features are collected\"\"\"\n",
    "        def _step(frames):\n",
    "            for f in frames:\n",
    "                f['xs'] += analyzer(f)\n",
    "                yield f\n",
    "        self.add_step(_step)\n",
    "\n",
    "    def __iter__(self):\n",
    "        pipeline = self._frames()\n",
    "        for s in self._steps:\n",
    "            pipeline = s(pipeline)\n",
    "        return pipeline\n",
    "    def _pprogress(self, count, force=False):\n",
    "        if force or time()-self._last>30:\n",
    "            self._last = time()\n",
    "            print(f\"{count+1} processed in {(time()-self._start)/60:2.1f} minutes\")\n",
    "    def extract_features(self):\n",
    "        self._start = time()\n",
    "        self._last = self._start\n",
    "        X = []\n",
    "        i=0\n",
    "        for i, f in enumerate(self):\n",
    "            X.append(f['xs'])\n",
    "            self._pprogress(i)\n",
    "        self._pprogress(i,True)\n",
    "        return X\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629 processed in 0.5 minutes\n",
      "1249 processed in 1.0 minutes\n",
      "1857 processed in 1.5 minutes\n",
      "2470 processed in 2.0 minutes\n",
      "3084 processed in 2.5 minutes\n",
      "3738 processed in 3.0 minutes\n",
      "4387 processed in 3.5 minutes\n",
      "5052 processed in 4.0 minutes\n",
      "5714 processed in 4.5 minutes\n",
      "6374 processed in 5.0 minutes\n",
      "7034 processed in 5.5 minutes\n",
      "7697 processed in 6.0 minutes\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureExtractor(frames)\n",
    "\n",
    "#fe.add_step(lambda g: islice(g, 17500, 20400, 1)) # limit frames (start, stop, step)\n",
    "#fe.add_step(lambda g: islice(g, 400, 420, 1)) # limit frames (start, stop, step)\n",
    "\n",
    "fe.add_processor(lambda img: crop(img, bottom=100, top=220))\n",
    "fe.add_processor(lambda img: cv2.cvtColor(img,cv2.COLOR_BGR2GRAY))\n",
    "fe.add_processor(fix_perspective)\n",
    "fe.add_processor(lambda img: cv2.GaussianBlur(img,(7,7),0))\n",
    "\n",
    "fe.add_step(lambda frames: lookahead(frames, count=3))\n",
    "\n",
    "#fe.add_analyzer(via_lk_optical_flow)\n",
    "fe.add_analyzer(via_lk_optical_flow_multi)\n",
    "fe.add_analyzer(via_variance_of_laplacian)\n",
    "fe.add_analyzer(via_diff)\n",
    "\n",
    "#fe.add_step(persist_raw_optical_flow)\n",
    "#fe.add_step(print_frame_keys)\n",
    "#fe.add_step(view_frames)\n",
    "\n",
    "xs = fe.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_time_in_chunks(df, n):\n",
    "    \"\"\"Break df into n-lengths mini dfs\"\"\"\n",
    "    assert len(df) >= n*10, \"doesn't meet minimum number of chunks\"\n",
    "    assert (len(df) % n) == 0, \"all chunks equal size\"\n",
    "    \n",
    "    chunk_count = len(df) // n\n",
    "    chunks = []\n",
    "    for x in range(0, len(df), n):\n",
    "        chunks.append(df[x:x + n])\n",
    "    random.shuffle(chunks)\n",
    "    print(f\"Using {len(chunks):0d} chunks\")\n",
    "    return pd.concat(chunks, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('../data/train.txt', header=None)\n",
    "X = pd.DataFrame(xs)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X.fillna(value=0, inplace=True)\n",
    "X.fillna(method='pad', inplace=True)\n",
    "#X=X[[0,2,4,3,'024']] # 1,2,3 frame Vf and 2-frame std, and linear average of the three Vf\n",
    "X=X[[0,1,2,  3,4,5,  6,7,8  ,9]]\n",
    "X=X[[3,4,5,9]]\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy = X.copy()\n",
    "Xy['y'] = y\n",
    "\n",
    "chunksize = 2040 # ten chunks\n",
    "chunksize = 204*2 # 50\n",
    "Xy = shuffle_time_in_chunks(Xy, chunksize)\n",
    "Xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_testcount(test_df, fraction_testset=0.3):\n",
    "    chunks = 1\n",
    "    while True:\n",
    "        chunks += 1\n",
    "        testcount = chunksize * chunks\n",
    "        if testcount/len(test_df)>fraction_testset:\n",
    "            break\n",
    "    print(f\"using testcount = {testcount}\")\n",
    "    return testcount\n",
    "    \n",
    "testcount = find_testcount(Xy, .43)\n",
    "\n",
    "Xy_train, Xy_test = train_test_split(Xy, test_size=testcount,shuffle=False)\n",
    "Xy_train, Xy_test = Xy_train.copy(deep=True), Xy_test.copy(deep=True)\n",
    "print(\"Percent test    =\", testcount/len(Xy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "m_dum = DummyRegressor(strategy='mean')\n",
    "\n",
    "m_ensemble = StackingRegressor([\n",
    "   ('svr', SVR(C=.3)),\n",
    "   ('rf', RandomForestRegressor(n_estimators=60)),\n",
    "   #('mlp', MLPRegressor(shuffle=False, alpha=0.1)),\n",
    "   ])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    #('poly', PolynomialFeatures(interaction_only=False, include_bias=False)),\n",
    "    #'reduce_dims', PCA(n_components=4)),\n",
    "    #('svr', SVR(C=2)),\n",
    "    #('rf', RandomForestRegressor()),\n",
    "    #'mlp', MLPRegressor(shuffle=False, alpha=0.1)),\n",
    "    #('br', BayesianRidge()),\n",
    "    ('stack', m_ensemble),\n",
    "    #('lin', LinearRegression()),\n",
    "    ])\n",
    "\n",
    "# # Training classifiers\n",
    "# reg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\n",
    "# reg2 = RandomForestRegressor(random_state=1, n_estimators=10)\n",
    "# reg3 = LinearRegression()\n",
    "# ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "# BaysianRidge\n",
    "# MLP NN\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'stack__svr__C':[.01, .1, .3, .9, 2, 4, 10, 15, 20, 50, 100, 600],\n",
    "    'stack__rf__n_estimators': [20, 50, 100, 150],\n",
    "    #'stack__rf__max_depth': None,\n",
    "    \n",
    "    #'svr__C':[10, 20, 30, 40, 70, 120],\n",
    "    #'mlp__alpha':[0.0001, 0.001, 0.01],\n",
    "    #'svr__C':[.3],RandomForestRegressor\n",
    "    #'svr__gamma':[0.006/4, 0.006/2,0.006,0.006*2],\n",
    "    }\n",
    "grid = GridSearchCV(pipe, parameters, verbose=10, n_jobs=5, cv=5)\n",
    "\n",
    "m_real = pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dum.fit(Xy_train[X.columns], Xy_train['y'])\n",
    "m_real.fit(Xy_train[X.columns], Xy_train['y'])\n",
    "try:\n",
    "    print(m_real.best_estimator_)\n",
    "except:\n",
    "    print(m_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate full original df\n",
    "Xy_test.loc[:,'is_test'] = True\n",
    "Xy_train.loc[:,'is_test'] = False\n",
    "Xy2 = pd.concat([Xy_test,Xy_train]).sort_index()\n",
    "\n",
    "# get predictions\n",
    "Xy2['dum'] = m_dum.predict(Xy2[X.columns])\n",
    "Xy2['pred'] = m_real.predict(Xy2[X.columns])\n",
    "\n",
    "# for convience\n",
    "Xy2_test_ix = Xy2['is_test'] == True\n",
    "Xy2_train_ix = Xy2['is_test'] == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# smooth based on distribution of acceleration in training set\n",
    "idea use kalman filter based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune a kalman filter\n",
    "from pykalman import KalmanFilter\n",
    "em_vars = [\n",
    "     #'transition_covariance',\n",
    "     'observation_covariance',\n",
    "     'initial_state_mean', 'initial_state_covariance']\n",
    "\n",
    "T = np.array([[.0009]]) # smaller is more resistance to acceleration\n",
    "\n",
    "kf = KalmanFilter(initial_state_mean=0, n_dim_obs=1, transition_covariance=T)\n",
    "kf_tuned = kf.em(Xy2.loc[Xy2_train_ix,'y'].values, n_iter=0, em_vars=em_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply kalman\n",
    "Xy2.loc[Xy2_test_ix,'pred_kf'] = kf_tuned.smooth(Xy2.loc[Xy2_test_ix,'pred'].values)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# never below 0\n",
    "Xy2.loc[Xy2['pred_kf'] < 0.0,'pred_kf'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mean = Xy2.loc[Xy2_train_ix,'y'].mean()\n",
    "target_mean = Xy2['y'].mean() # for full comparison, overly optimistic though\n",
    "\n",
    "NoMLCol = 9\n",
    "\n",
    "_ = kf_tuned.smooth(Xy2[NoMLCol].values)[0]\n",
    "Xy2['NoML_kf'] = _ * target_mean/_.mean()\n",
    "Xy2['NoML_scaled'] = Xy2[NoMLCol] * target_mean/Xy2[NoMLCol].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [100, 12]\n",
    "plt.gca().set_xlim((0,len(Xy2)))\n",
    "def plot_Xy2(ix_mask, column, **kwargs):\n",
    "    try:\n",
    "        if ix_mask is None:\n",
    "            df = Xy2.loc[:,column]\n",
    "        else:\n",
    "            df = Xy2.loc[ix_mask, column]\n",
    "        plt.plot(df.index, df.values, **kwargs)\n",
    "    except KeyError:\n",
    "        print(f\"Skipping {column}\")\n",
    "    \n",
    "#plot_Xy2(Xy2_test_ix, 'pred', marker='o', linewidth=0.0, color='green', alpha=.1)\n",
    "#plot_Xy2(Xy2_train_ix, 'pred', marker='o', linewidth=0.0, color='purple', alpha=.1)\n",
    "plot_Xy2(None, 'NoML_scaled', marker='o', linewidth=0.0, color='yellow', alpha=.1)\n",
    "plot_Xy2(None, 'NoML_kf', marker='', linewidth=1.5, color='orange')\n",
    "plot_Xy2(None, '1_thresh', marker='', linewidth=0.5, color='blue')\n",
    "plot_Xy2(None, 'y', marker='', linewidth=1.4, color='red')\n",
    "plot_Xy2(Xy2_test_ix, 'pred_kf', marker='o', linewidth=0.0, color='green', alpha=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_line(ix_mask, column='dum'):\n",
    "    if ix_mask is not None:\n",
    "        Xy = Xy2[ix_mask]\n",
    "    else:\n",
    "        Xy = Xy2\n",
    "    err = mean_squared_error(Xy['y'], Xy[column])\n",
    "    print(f\"{err:0.1f}\", end='\\t')\n",
    "\n",
    "print(f\"dummy\\ttest\\ttrain\\ttest_kf\\tNoML\")\n",
    "print_summary_line(Xy2_test_ix,'dum')\n",
    "print_summary_line(Xy2_test_ix,'pred')\n",
    "print_summary_line(Xy2_train_ix,'pred')\n",
    "print_summary_line(Xy2_test_ix,'pred_kf')\n",
    "print_summary_line(None,'NoML_kf') # 10.3 was best, gaussian made it 7.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dummy\ttest\ttrain\ttest_kf\tNoML\n",
    "69.6\t16.6\t5.6\t7.7\t13.5\t\n",
    "\n",
    "^^^^got this,,,,,why>???????? trying to lower absurity ceiling again 260->160\n",
    "maybe column multiplication should be done _after_ forward fillign/padding Xdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Xy2.copy()\n",
    "df = df.groupby(np.arange(len(df.index)) // 100).mean()\n",
    "df['loss']=abs(df['y']-df['NoML'])\n",
    "#plt.plot(df.index, df.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='loss', ascending=False).head(10).index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be sure pipeline is set up the same in both places\n",
    "columns_used_to_train = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feo = FeatureExtractor(lambda: frames(file='../data/train.mp4'))\n",
    "\n",
    "#feo.add_step(lambda g: islice(g, 17500, 20400, 1)) # limit frames (start, stop, step)\n",
    "#feo.add_step(lambda g: islice(g, 400, 420, 1)) # limit frames (start, stop, step)\n",
    "\n",
    "feo.add_processor(lambda img: crop(img, bottom=100, top=220))\n",
    "feo.add_processor(lambda img: cv2.cvtColor(img,cv2.COLOR_BGR2GRAY))\n",
    "feo.add_processor(fix_perspective)\n",
    "feo.add_processor(lambda img: cv2.GaussianBlur(img,(7,7),0))\n",
    "\n",
    "feo.add_step(lambda frames: lookahead(frames, count=3))\n",
    "\n",
    "#feo.add_analyzer(via_lk_optical_flow)\n",
    "feo.add_analyzer(via_lk_optical_flow_multi)\n",
    "feo.add_analyzer(via_variance_of_laplacian)\n",
    "feo.add_analyzer(via_diff)\n",
    "\n",
    "#feo.add_step(print_frame_keys)\n",
    "#feo.add_step(view_frames)\n",
    "\n",
    "xso = fe.extract_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_used_to_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
